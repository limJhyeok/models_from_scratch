{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my github: https://github.com/withAnewWorld/models_from_scratch\n",
    "# my blog\n",
    "# https://self-deeplearning.blogspot.com/\n",
    "# https://self-deeplearning.tistory.com/\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usWCwHGLn8AX"
   },
   "source": [
    "## Ref\n",
    "1. Efficient Net <br>\n",
    "  paper: https://arxiv.org/abs/1905.11946<br>\n",
    "  \n",
    "  official PyTorch code: https://github.com/pytorch/vision/blob/main/torchvision/models/efficientnet.py\n",
    "\n",
    "  how to make EfficientNet with PyTorch(Youtuber: \n",
    "Aladdin Persson): https://www.youtube.com/watch?v=fR_0o25kigM&list=PLhhyoLH6IjfxeoooqP9rhU3HJIAVAJ3Vz&index=20 <br>\n",
    "\n",
    "  blog: https://lynnshin.tistory.com/53, https://lynnshin.tistory.com/13<br>\n",
    "\n",
    "  architecture picture: https://www.researchgate.net/figure/The-structure-of-an-EfficientNetB0-model-with-the-internal-structure-of-MBConv1-and_fig2_351057828 <br>\n",
    "\n",
    "2. Mobile Net v2 <br>\n",
    "  paper: https://arxiv.org/abs/1801.04381<br>\n",
    "  blog(depthwise conv): https://coding-yoon.tistory.com/122 <br>\n",
    "\n",
    "3. SE Net <br>\n",
    "  paper: https://arxiv.org/abs/1709.01507<br>\n",
    "  blog: https://deep-learning-study.tistory.com/539 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 목차\n",
    "1. Efficient net 탄생배경 <br>\n",
    "2. Mobile Net v2 \n",
    "    - depthwise conv <br>\n",
    "    - depthwise separable conv <br>\n",
    "    - inverted residual block <br>\n",
    "3. SENet\n",
    "4. Efficient Net Base Model(EfficientNet B0)\n",
    "5. Compound tune\n",
    "6. Efficient Net(B0~B7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3XDe-uaADuU"
   },
   "source": [
    "## EfficientNet\n",
    "탄생배경 <br>\n",
    "model architecture에 대한 연구는 활발히 진행 되어왔지만 해당 모델이 architecture에서 최대의 성능을 내는 것인지 확인하기 어려움 <br>\n",
    "(ex. VGG, ResNet, GoogLeNet, ...)<br>\n",
    "ex) ResNet의 경우 최적의 성능을 내는 depth를 어떻게 설정할 것인가? <br>\n",
    "\n",
    "기존 연구에서 개별 변수(모델 layer의 깊이, input image의 크기(Height X Width), 각 layer의 out channel)에 대한 연구는 진행된 적있지만 이들을 통합적으로 고려한 연구는 진행되지 않았음 <br>\n",
    "\n",
    "EfficientNet은\n",
    "> 1) model layer의 width(out channel) <br>\n",
    "> 2) model layer의 depth <br>\n",
    "> 3) input image의 resolution(Height x Width) <br>\n",
    "\n",
    "을 동시에 조절(compound scaling)하여 최적의 조합을 찾고자 함(주어진 컴퓨터 자원하에서) <br>\n",
    "\n",
    "즉, 목적함수: <br>\n",
    "\n",
    "$max\\ Accuracy(network)$ <br>\n",
    "(variable: depth, width, resolution) <br>\n",
    "\n",
    "s.t) <br>\n",
    "Memory(network) <= target_memory <br>\n",
    "Flops(network) <= target_flops <br>\n",
    "cf) flops: 초당 부동소수점 연산 <br>\n",
    "<br>\n",
    "notebook 진행 순서 <br>\n",
    "\n",
    "1) base model 생성 (EfficientNet-B0) <br>\n",
    "> MobileNet v2 설명, 일부 구현<br>\n",
    "> SENet 설명, 구현<br>\n",
    "\n",
    "2) compound scaling\n",
    "\n",
    "작성자 blog: https://self-deeplearning.blogspot.com/ <br>\n",
    "작성자 github: https://github.com/withAnewWorld/models_from_scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWUqV4sW2H9j"
   },
   "source": [
    "Efficient Net의 경우 base model을 생성하기 위해 <br>\n",
    "1) Moblie Net v1, v2에서 소개된 depth wise convolution, inverted residual connnection <br>\n",
    "2) SENet <br>\n",
    "3) Stochastic Depth(생략) <br>\n",
    "을 구현할 줄 알아야 합니다. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYnaLTsSprty"
   },
   "source": [
    "## Mobile Net v2\n",
    "모바일 기기(ex.스마트폰) 환경에서 deep learning을 구현하기 위해 고안된 Mobile net의 version 2 <br>\n",
    "* 특징 <br>\n",
    "1) 모바일 환경에서 작동해야하는 만큼 메모리 부하가 적어야 한다. <br>\n",
    "2) 적은 computing 자원으로도 빠르게 결과를 도출해야 한다. <br>\n",
    " -> 연산 효율성 극대화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/withAnewWorld/models_from_scratch/main/pic/overview_mobilenet_v2.png\">\n",
    "</p>\n",
    "\n",
    "<div align='center'> paper table 2 (overview of mobile net v2 architecture) </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQKfxf_B56Es"
   },
   "source": [
    "MobileNet v2에서 모르실만한 모듈은 bottleneck일 것입니다. <br>\n",
    "bottleneck module은 다음과 같이 이루어져 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/withAnewWorld/models_from_scratch/main/pic/bottleneck.png\">\n",
    "</p>\n",
    "\n",
    "<div align='center'> paper table 1 (architecture of bottleneck) </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/withAnewWorld/models_from_scratch/main/pic/depthwise_conv.png\">\n",
    "</p>\n",
    "\n",
    "<div align='center'> 자료출처: https://coding-yoon.tistory.com/122 </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/withAnewWorld/models_from_scratch/main/pic/depthwise.png\">\n",
    "</p>\n",
    "\n",
    "<div align='center'> 자료출처: https://cs231n.github.io/convolutional-networks/ (일부 수정)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwxrfpZs6Nft"
   },
   "source": [
    "## depthwise conv\n",
    "기존의 conv net의 경우 input(C, H, W)의 모든 C(channel)에 대해 weight sum을 진행. <br>\n",
    "반면 depthwise conv net의 경우 input(C, H, W)를 각 C(channel)로 쪼갠 후(1, H, W) <br>\n",
    "convolution 연산 진행, 결과값 concat\n",
    "```python\n",
    "# pseudo code\n",
    "def conv():\n",
    "  '''\n",
    "  input(Tensor[N, C, H, W]) (N: num_imgs, C: channels, H: height, W: width)\n",
    "  filter(Tensor[F, C, HH, WW]) (F: num_filters, C: channels, HH: filter height, WW: filter width)\n",
    "  '''\n",
    "  one_feature_map\n",
    "  while:\n",
    "    sum(input[N, :, stride:stride+HH, stride:stride+WW] * filter[F, :, :, :])\n",
    "    slide window(stride)\n",
    "\n",
    "def depthwise_conv():\n",
    "  one_feature_map\n",
    "  while:\n",
    "    output[i] = sum(input[N, i, stride:stride+HH, stride:stride+WW] * filter[F, i, :, :])\n",
    "    slide window(stride)\n",
    "\n",
    "```\n",
    "Why depthwise conv? <br>\n",
    "기존 conv의 경우 하나의 feature map을 계산하기 위해 <br>\n",
    "kernel_size * kernel_size * in_channels의 parameters 필요. <br>\n",
    "\n",
    "반면 depthwise conv의 경우 <br>\n",
    "kernel_size * kernel_size의 parameters 필요 <br>\n",
    "\n",
    "-> 연산량 효율적 감소\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "I6EIulYUbLkP"
   },
   "outputs": [],
   "source": [
    "class DepthWiseConv(nn.Module): # we will not use this module. It's just for explanation\n",
    "  def __init__(self, in_channels = 3, kernel_size = 3, stride = 2):\n",
    "    super(DepthWiseConv, self).__init__()\n",
    "    '''\n",
    "    spilt input tensor into [N, 1, H, W]\n",
    "    and then apply conv(In_channels = 1, out_chacnnels = 1)\n",
    "    last concat outputs\n",
    "    input\n",
    "      - input_tensor: Tensor[N, C, H, W] \n",
    "      - kernel_size(int)\n",
    "    output\n",
    "      - concat((Tensors), dim = 1))\n",
    "        * Tensors(List): [Tensor[N, 1, H', W'], Tensor[N, 1, H', W'], ...]\n",
    "        * len(Tensors) = C\n",
    "    '''\n",
    "    if kernel_size == 3:\n",
    "      padding = 1\n",
    "    elif kernel_size == 5:\n",
    "      padding = 2\n",
    "    self.convs = nn.ModuleList([nn.Conv2d(in_channels = 1,\n",
    "                                out_channels = 1,\n",
    "                                kernel_size = kernel_size,\n",
    "                                stride = stride,\n",
    "                                padding = padding,\n",
    "                                bias = False) for i in range(in_channels)]) # conv2d groups keyward를 통해 쉽게 depth wise conv 가능\n",
    "\n",
    "  def forward(self ,x):\n",
    "    x_split = torch.split(x, 1, dim=1)\n",
    "  \n",
    "    output = [conv(x_split[i]) for i, conv in enumerate(self.convs)]\n",
    "\n",
    "    return torch.cat(output, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "uNGjGZ6He_u_"
   },
   "outputs": [],
   "source": [
    "x = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "model = DepthWiseConv(3)\n",
    "depth_wise_groups = nn.Conv2d(in_channels = x.size(1), # groups keyword를 활용한 depthwise conv net\n",
    "                             out_channels = x.size(1),\n",
    "                             kernel_size = (3, 3),\n",
    "                             stride = 2,\n",
    "                             padding = 1,\n",
    "                             groups = x.size(1),\n",
    "                             bias = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "QJ4Av7y1fEya"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  for i in range(x.size(1)):\n",
    "    depth_wise_groups.weight[i] = model.convs[i].weight  # for matching each of conv parameters\n",
    "\n",
    "out = model(x)\n",
    "correct_out = depth_wise_groups(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yxZxEB7ScOAA",
    "outputId": "38b87402-ec5c-460e-e579-a49e105222ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(out, correct_out) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "cYwH6xyRtuv5"
   },
   "outputs": [],
   "source": [
    "class DepthWiseConv(nn.Module): \n",
    "  def __init__(self, in_channels = 3, kernel_size = 3, stride = 2):\n",
    "    super(DepthWiseConv, self).__init__()\n",
    "    '''\n",
    "    inputs\n",
    "      - input_tensor: Tensor[N, C, H, W]\n",
    "      - kernel_size(int)\n",
    "      - stride(int)\n",
    "    outputs\n",
    "      - Tensor[N, C, H, W]\n",
    "    '''\n",
    "    if kernel_size == 3:\n",
    "      padding = 1\n",
    "    elif kernel_size == 5:\n",
    "      padding = 2\n",
    "    self.conv = nn.Conv2d(in_channels = in_channels,\n",
    "                           out_channels = in_channels,\n",
    "                           kernel_size = kernel_size,\n",
    "                           stride = stride,\n",
    "                           padding = padding,\n",
    "                           groups = in_channels)\n",
    "\n",
    "  def forward(self ,x):\n",
    "    return self.conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C9RuH9xWpk_U"
   },
   "source": [
    "## depthwise separable conv\n",
    "두 개의 module로 구성 <br>\n",
    "1) depthwise conv <br>\n",
    "2) pointwise conv(1x1 conv) <br>\n",
    "(1x1 conv)의 경우 주로 input tensor의 channel을 변경하고자 할 때 많이 사용 <br>\n",
    "pointwise conv의 경우 연산량이 매우 적고 depthwise conv의 경우 상대적으로 더 연산량이 많음. <br>\n",
    "\n",
    "* depthwise separable conv 목적 <br>\n",
    "이에 따라 pointwise conv을 이용해 연산량을 최대한 줄이고 depthwise conv을 통해 feature 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/withAnewWorld/models_from_scratch/main/pic/depthwise_separable.png\">\n",
    "</p>\n",
    "\n",
    "<div align='center'> 자료출처: https://coding-yoon.tistory.com/122 </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Y3GIbA2Ei3YO"
   },
   "outputs": [],
   "source": [
    "class PointwiseConv(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels):\n",
    "    super().__init__()\n",
    "    self.pointwise = nn.Conv2d(in_channels = in_channels,\n",
    "                               out_channels = out_channels,\n",
    "                               kernel_size = (1, 1),\n",
    "                               stride = 1)\n",
    "  def forward(self, x):\n",
    "    return self.pointwise(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SAjQiQe874TC"
   },
   "source": [
    "## Inverted Residual Block\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/withAnewWorld/models_from_scratch/main/pic/inverted_residual_block.png\">\n",
    "</p>\n",
    "\n",
    "<div align='center'> mobilenetV2 paper figure3 </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLmoso-V8kaP"
   },
   "source": [
    "기존의 residual block의 경우 <br>\n",
    "(wide -> narrow -> wide) <br>\n",
    "> input의 channel 감소 (1x1 conv) & activation <br>\n",
    "> conv & activation <br>\n",
    "> channel 증가(1x1 conv) & activation<br>\n",
    "> skip connection (x+=identity) <br>\n",
    "\n",
    "inverted residual block <br>\n",
    "(narrow -> wide -> narrow) \n",
    "> input channel 증가 & activation<br>\n",
    "> conv & activation <br>\n",
    "> channel 감소 (1x1 conv) & activation<br>\n",
    "> skip connection (x+=identity) <br>\n",
    "-> 목적: 더 적은 channel의 tensor를 connection 함으로써 연산량 감소\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Rgh5O-FnjSfS"
   },
   "outputs": [],
   "source": [
    "class LinearBottleneck(nn.Module):\n",
    "  def __init__(self, in_channels, expansion, out_channels, stride):\n",
    "    super().__init__()\n",
    "    self.in_channels = self.in_channels\n",
    "    self.out_channels = self.out_channels\n",
    "    self.stride = stride\n",
    "\n",
    "    self.pointwise = pointwise_conv(in_channels, expansion)\n",
    "    self.depth_wise_conv = depth_wise_conv(in_channels = expansion, kernel_size = 3, stride = stride)  \n",
    "    self.proj = pointwise_conv(expansion, out_channels)\n",
    "  def forward(self, x):\n",
    "    identity = x\n",
    "    x = self.pointwise(x)\n",
    "    x = self.depth_wise_conv(x)\n",
    "    x = self.proj(x)\n",
    "    # inverted residual block\n",
    "    if self.in_channels == self.out_channels and self.stride == 1:\n",
    "      x += identity\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z7406lVZFVKV"
   },
   "source": [
    "## SENet(Squeeze-and-Excitation Networks)\n",
    "탄생 배경 <br>\n",
    "기존 conv net의 경우 input tensor(C, H, W)에 대해 각 필터(Filter(C, H', W'))를 weight sum하므로 모든 channel의 정보가 하나로 담겨집니다.\n",
    "```python\n",
    "def conv(input):\n",
    "  '''\n",
    "  input(Tensor[N, C, H, W])\n",
    "  filter(Tensor[F, C, HH, WW])\n",
    "  '''\n",
    "  while:\n",
    "    sum(input[N, :, stride:stride+HH, stride:stride+WW] * filter[F, :, :, :])\n",
    "    slide window(stride)\n",
    "```\n",
    "따라서 각 channel간의 상호작용에 대한 정보가 명시적(explicitly)으로 담기지 않게됩니다. <br>\n",
    "이를 명시적으로 담기 위해 각 채널에 대한 정보를 압축한 후 excitation하여 Input tensor에 곱합니다.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/withAnewWorld/models_from_scratch/main/pic/SE_block.png\">\n",
    "</p>\n",
    "\n",
    "<div align = 'center'> SENet paper fig. 1</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WN_sKXcDh8F"
   },
   "source": [
    ">1) 먼저 Squeeze layer를 통해 이전 layer를 거친 feature tensor(H, W, C)를 (1, 1, C)로 압축한 후 <br>\n",
    ">2) Excitaiton layer를 통해 압축된 tensor에 projection & activation function을 적용합니다. <br>\n",
    ">3) 해당 결과값(1, 1, C)을 feature tensor(H, W, C)에 곱합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/withAnewWorld/models_from_scratch/main/pic/SENet.png\">\n",
    "</p>\n",
    "\n",
    "<div align = 'center'> detail architecture of SENet(base model: Inception Net) from SENet paper</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpA9kGNMt4o-"
   },
   "source": [
    "## Squeeze\n",
    "```python\n",
    "# pseudo code\n",
    "def Squeeze():\n",
    "  inputs:\n",
    "    feature(Tensor[N, C, H, W])\n",
    "  returns:\n",
    "    output(Tensor[N, C, 1, 1])\n",
    "```\n",
    "\n",
    "1. Squeeze를 위해 무조건 Average pool 사용? <br>\n",
    "저자는 maxpool과 average pool을 실험에 사용했지만 Average pool이 더 좋은 성능을 보이는 것을 확인. <br>\n",
    "(SENet paper TABLE 11 참조) <br>\n",
    "모델마다 적합한 Squeeze 방법론이 다를 수 있으므로 실험 또는 이론적으로 다른 알고리즘을 적용해보고 특별한 경우가 아닐 경우 average pool 적용 <br>\n",
    "\n",
    "## Excitation\n",
    "(1, 1, C) -> (1, 1, C/r) -> (1, 1, C) <br>\n",
    "```python\n",
    "# pseudo code\n",
    "def Excitation():\n",
    "  Linear(C, C/r)\n",
    "  relu()\n",
    "  Linear(C/r, C)\n",
    "  sigmoid()\n",
    "```\n",
    "1. Why Excitation(Linear & activation) module 적용? <br>\n",
    "1) feature의 pooling 값을 그냥 곱할 경우 학습이 적절하게 되지 않을 가능성. <br>\n",
    "즉 적절한 learnable parameter 필요 <br>\n",
    "2) 각 채널간의 상호작용에 대한 정보 필요 <br>\n",
    "  -> fully connected layer and activation function <br>\n",
    "2. Why channel reduction(r)?<br>\n",
    "1) 연산량을 줄이기 위해<br>\n",
    "2) 일반화를 위해 <br>\n",
    "\n",
    "```python\n",
    "# psudo code\n",
    "def SENet():\n",
    "  SE_block = (squeeze & excitation) # SE_block: 각 채널의 상호작용에 대한 정보 \n",
    "  return x * SE_block(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ONjnpH2EnHha"
   },
   "outputs": [],
   "source": [
    "class Squeeze(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Squeeze, self).__init__()\n",
    "    self.GAP = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.GAP(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "KLpIOSkxB6TE"
   },
   "outputs": [],
   "source": [
    "class Excitation(nn.Module):\n",
    "  def __init__(self, in_channels, r = 4):\n",
    "    '''\n",
    "    inputs\n",
    "      - in_channels\n",
    "      - r(int): channel reduction ratio\n",
    "    '''\n",
    "    super(Excitation, self).__init__()\n",
    "    self.block = nn.Sequential(\n",
    "        nn.Conv2d(in_channels = in_channels,\n",
    "                  out_channels = int(in_channels/r),\n",
    "                  kernel_size = (1, 1),\n",
    "                  stride = 1),\n",
    "        nn.ReLU(), \n",
    "        nn.Conv2d(in_channels = int(in_channels/r),\n",
    "                  out_channels = in_channels,\n",
    "                  kernel_size = (1, 1),\n",
    "                  stride = 1),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "    \n",
    "  def forward(self, x):\n",
    "    return self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ZhW8Q1pdC1Zf"
   },
   "outputs": [],
   "source": [
    "class SENet(nn.Module):\n",
    "  def __init__(self, in_channels, r = 4):\n",
    "    super(SENet, self).__init__()\n",
    "    self.SE_block = nn.Sequential(\n",
    "        Squeeze(),\n",
    "        Excitation(in_channels, r)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    output = self.SE_block(x)\n",
    "    return x * output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UxI_Qx8yDo7V",
    "outputId": "5fe22f10-9a91-4eb1-9a18-61e4e952e604"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 7, 7])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 128, 7, 7)\n",
    "model = SENet(in_channels = 128, r = 4)\n",
    "output = model(x) # output.size() = (1, 128, 7, 7)\n",
    "output.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0UFYjQ-YFglh"
   },
   "source": [
    "## Efficient Net base model(EfficientNetB0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/withAnewWorld/models_from_scratch/main/pic/EfficientNetB0_table1.png\">\n",
    "</p>\n",
    "\n",
    "<div align = 'center'> architecture of base model b0 (from EfficientNet paper table 1)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/withAnewWorld/models_from_scratch/main/pic/detail_EfficientNet.png\">\n",
    "</p>\n",
    "\n",
    "<div align = 'center'> 자료출처: https://www.researchgate.net/figure/The-structure-of-an-EfficientNetB0-model-with-the-internal-structure-of-MBConv1-and_fig2_351057828</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "do5H7dlMw54N"
   },
   "source": [
    "## EfficidentNet-B0 basline network\n",
    "위의 모델 architecture에 따라 EfficientNet-B0를 설계하고자 할 때, <br>\n",
    "\n",
    "1. MBConv의 kernel_size가 가변적으로 변하므로 kernel_size를 변수로 설정합니다 <br>\n",
    "\n",
    "2. MBConv1과 MBConv6 두 모듈을 나눠서 설계합니다. <br>\n",
    "\n",
    "3. 각 layer의 결과값을 살펴보면 output의 resolution(H*W)을 결정짓는 것은 MBConv 내의 depthwise conv입니다. <br>\n",
    "\n",
    "  conv layer의 resolution은 크게 kenel_size, stride, padding에 영향을 받습니다. <br>\n",
    "  여기서 layer output을 살펴보면 kerenl_size에 따라 resolution이 결정되지 않습니다.<br>\n",
    "  또한 해당 모델에서 padding은 resolution을 유지하는 역할을 하므로 stride를 통해 image의 resolution을 제어합니다. <br>\n",
    "  cf) $w' = 1 + (w-kernel\\ size + 2*padding)/(2*stride)$ <br>\n",
    "  (w: width, w': width after conv, in case of no dilation )\n",
    "\n",
    "4. MBConv block의 경우 #layer가 1 이상이 가능. 따라서, <br>\n",
    "1) 처음 MBconv layer의 경우 in channel을 이전 MBConv layer의 out channel로 설정 <br>\n",
    "2) 다음 layer에서 kenrel size 유지(stride: 1), in channel = out channel\n",
    "```python\n",
    "# pseudo code\n",
    "class EfficientNetB0(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(EfficientNetB0, self).__init__()\n",
    "    \n",
    "    self.stage_1 = conv3x3(stride:1, in_channels = 3, out_channels = 32)\n",
    "    self.stage_2 = MBConv1_3x3(stride:2, in_channels = 32, out_channels = 16)\n",
    "    self.stage_3 = [MBConv6_3x3(stride:1, in_channels = 16, out_channels = 24),\n",
    "    MBConv6_3x3(stride:1, in_channels = 24, out_channels = 24]\n",
    "    self.stage_4 = [MBConv6_5x5(stride:2, in_channels = 24, out_channels = 40),\n",
    "    MBConv6_5x5(stride:1, in_channels = 40, out_channels = 40]\n",
    "    self.stage_5 = [MBConv6_3x3(stride:2, in_channels = 40, out_channels = 80,\n",
    "    MBConv6_3x3(stride:1, in_channels = 80, out_channels = 80),\n",
    "    MBConv6_3x3(stride:1, in_channels = 80, out_channels = 80)]\n",
    "    self.stage_6 = [\n",
    "    MBConv6_5x5(stride:2, in_channels = 80, out_channels = 112),\n",
    "    MBConv6_5x5(stride:1, in_channels = 112, out_channels = 112),\n",
    "    MBConv6_5x5(stride:1, in_channels = 112, out_channels = 112)]\n",
    "    \n",
    "    ...\n",
    "\n",
    "    self.stage_9 = [\n",
    "      conv1x1,\n",
    "      pool,\n",
    "      flatten,\n",
    "      fc\n",
    "    ]\n",
    "  def forward(self, x):\n",
    "    for stage:\n",
    "      x = self.stage(x)\n",
    "    return x\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "zkIQZtMbH7C6"
   },
   "outputs": [],
   "source": [
    "class Swish(nn.Module): # search torch.nn.SiLU()\n",
    "  def __init__(self):\n",
    "    super(Swish, self).__init__()\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, x):\n",
    "    return x * self.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "1y8VzlyiIKDN"
   },
   "outputs": [],
   "source": [
    "class MBConv1(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, kernel_size, stride, r = 4):\n",
    "    '''\n",
    "    inputs:\n",
    "      - in_channels(int)\n",
    "      - out_channels(int)\n",
    "      - kernel_size(int): 3: will get padding 1, 5: will get padding 5 in depth wise conv\n",
    "      - stride(int): 1: will retrun same resolution(H*W), 2: will return resolution/4 (H/2 * W/2)\n",
    "      - r(int): channel reduction ratio of SENet\n",
    "    returns:\n",
    "      - MBConv1\n",
    "    '''\n",
    "    super(MBConv1, self).__init__()\n",
    "    self.in_channels = in_channels\n",
    "    self.out_channels = out_channels\n",
    "    self.stride = stride\n",
    "    self.block = nn.Sequential(\n",
    "     DepthWiseConv(in_channels = in_channels, \n",
    "                     kernel_size = kernel_size,\n",
    "                     stride = stride),\n",
    "     nn.BatchNorm2d(num_features = in_channels),\n",
    "     Swish(),\n",
    "     SENet(in_channels = in_channels, r = 4),\n",
    "     nn.Conv2d(in_channels = in_channels,\n",
    "               out_channels = out_channels,\n",
    "               kernel_size = (1, 1),\n",
    "               stride = 1),\n",
    "      nn.BatchNorm2d(num_features = out_channels)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''\n",
    "    input:\n",
    "      -  x(Tensor[N, in_C, H, W])\n",
    "    return:\n",
    "      -  x(Tensor[N, out_C, H', W']): stride 1: H', W' = H, W | stride 2: H', W' = H/2, W/2\n",
    "    '''\n",
    "    # inverted residual block\n",
    "    if self.in_channels == self.out_channels and self.stride == 1:\n",
    "      identity = x\n",
    "      x = self.block(x) \n",
    "      x += identity\n",
    "    else:\n",
    "      x = self.block(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "f22LTeLnPM4J"
   },
   "outputs": [],
   "source": [
    "class MBConv6(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, kernel_size, stride, r = 4):\n",
    "    '''\n",
    "    inputs:\n",
    "      - in_channels(int)\n",
    "      - out_channels(int)\n",
    "      - kernel_size(int): 3: will get padding 1, 5: will get padding 5 in depth wise conv\n",
    "      - stride(int): 1: will retrun same resolution(H*W), 2: will return half resolution(H/2 * W/2)\n",
    "      - r(int): channel reduction ratio of SENet\n",
    "    returns:\n",
    "      - MBConv6 (6 means channel expansion)\n",
    "    '''\n",
    "    \n",
    "    super(MBConv6, self).__init__()\n",
    "    self.in_channels = in_channels\n",
    "    self.out_channels = out_channels\n",
    "    self.stride = stride\n",
    "    self.block = nn.Sequential(\n",
    "     nn.Conv2d(in_channels = in_channels,\n",
    "               out_channels = 6*in_channels,\n",
    "               kernel_size = (1, 1),\n",
    "               stride = 1),\n",
    "     nn.BatchNorm2d(num_features = 6*in_channels),\n",
    "     Swish(),\n",
    "     DepthWiseConv(in_channels = 6*in_channels, \n",
    "                     kernel_size = kernel_size, \n",
    "                     stride = stride),\n",
    "     nn.BatchNorm2d(num_features = 6*in_channels),\n",
    "     Swish(),\n",
    "     SENet(in_channels = 6*in_channels, r = 4),\n",
    "     nn.Conv2d(in_channels = 6*in_channels,\n",
    "               out_channels = out_channels,\n",
    "               kernel_size = (1, 1),\n",
    "               stride = 1),\n",
    "      nn.BatchNorm2d(num_features = out_channels)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''\n",
    "    input:\n",
    "      -  x(Tensor[N, in_C, H, W])\n",
    "    return:\n",
    "      -  x(Tensor[N, out_C, H', W']): stride 1: H', W' = H, W | stride 2: H', W' = H/2, W/2\n",
    "    '''\n",
    "\n",
    "    # inverted residual block\n",
    "    if self.in_channels == self.out_channels and self.stride == 1:\n",
    "      identity = x\n",
    "      x = self.block(x) \n",
    "      x += identity \n",
    "    else:\n",
    "      x = self.block(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "oi_FoZDDFiz4"
   },
   "outputs": [],
   "source": [
    "class EfficientNet(nn.Module):\n",
    "  def __init__(self, img_channels, num_classes):\n",
    "    super(EfficientNet, self).__init__()\n",
    "    self.stage_1 = nn.Conv2d(in_channels = img_channels,\n",
    "                          out_channels = 32,\n",
    "                          kernel_size = (3, 3),\n",
    "                          stride = 1,\n",
    "                          padding = 1)\n",
    "    \n",
    "    self.stage_2 = MBConv1(in_channels = 32,\n",
    "                           out_channels = 16,\n",
    "                           kernel_size = 3,\n",
    "                           stride = 2)\n",
    "    \n",
    "    self.stage_3 = [MBConv6(in_channels = 16,\n",
    "                           out_channels = 24,\n",
    "                           kernel_size = 3,\n",
    "                           stride = 1),\n",
    "                    MBConv6(in_channels = 24,\n",
    "                            out_channels = 24,\n",
    "                            kernel_size = 3,\n",
    "                            stride = 1)]\n",
    "    self.stage_3 = nn.Sequential(*self.stage_3)\n",
    "\n",
    "    self.stage_4 = [MBConv6(in_channels = 24,\n",
    "                            out_channels = 40,\n",
    "                            kernel_size = 5,\n",
    "                            stride = 2),\n",
    "                    MBConv6(in_channels = 40,\n",
    "                            out_channels = 40,\n",
    "                            kernel_size = 5,\n",
    "                            stride = 1)]\n",
    "    self.stage_4 = nn.Sequential(*self.stage_4)\n",
    "\n",
    "    self.stage_5 = [MBConv6(in_channels = 40,\n",
    "                            out_channels = 80,\n",
    "                            kernel_size = 3,\n",
    "                            stride = 2),\n",
    "                    MBConv6(in_channels = 80,\n",
    "                            out_channels = 80,\n",
    "                            kernel_size = 3,\n",
    "                            stride = 1),\n",
    "                    MBConv6(in_channels = 80,\n",
    "                            out_channels = 80,\n",
    "                            kernel_size = 3,\n",
    "                            stride = 1)]\n",
    "    self.stage_5 = nn.Sequential(*self.stage_5)\n",
    "\n",
    "    self.stage_6 = [MBConv6(in_channels = 80,\n",
    "                            out_channels = 112,\n",
    "                            kernel_size = 5,\n",
    "                            stride = 2),\n",
    "                    MBConv6(in_channels = 112,\n",
    "                            out_channels = 112,\n",
    "                            kernel_size = 5,\n",
    "                            stride = 1),\n",
    "                    MBConv6(in_channels = 112,\n",
    "                            out_channels = 112,\n",
    "                            kernel_size = 5,\n",
    "                            stride = 1)]\n",
    "    self.stage_6 = nn.Sequential(*self.stage_6)\n",
    "\n",
    "    self.stage_7 = [MBConv6(in_channels = 112,\n",
    "                            out_channels = 192,\n",
    "                            kernel_size = 5,\n",
    "                            stride = 1),\n",
    "                    MBConv6(in_channels = 192,\n",
    "                            out_channels = 192,\n",
    "                            kernel_size = 5,\n",
    "                            stride = 1),\n",
    "                    MBConv6(in_channels = 192,\n",
    "                            out_channels = 192,\n",
    "                            kernel_size = 5,\n",
    "                            stride = 1),\n",
    "                    MBConv6(in_channels = 192,\n",
    "                            out_channels = 192,\n",
    "                            kernel_size = 5,\n",
    "                            stride = 1),\n",
    "                    MBConv6(in_channels = 192,\n",
    "                            out_channels = 192,\n",
    "                            kernel_size = 5,\n",
    "                            stride = 1)]\n",
    "    self.stage_7 = nn.Sequential(*self.stage_7)\n",
    "\n",
    "    self.stage_8 = MBConv6(in_channels = 192,\n",
    "                           out_channels = 320,\n",
    "                           kernel_size = 3,\n",
    "                           stride = 2)\n",
    "    \n",
    "    self.stage_9 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels = 320,\n",
    "                  out_channels = 1280,\n",
    "                  kernel_size = (1, 1),\n",
    "                  stride = 1),\n",
    "        nn.AdaptiveMaxPool2d(1),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(in_features = 1280,\n",
    "                  out_features = num_classes)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.stage_1(x)\n",
    "    print(x.size())\n",
    "    x = self.stage_2(x)\n",
    "    print(x.size())\n",
    "    x = self.stage_3(x)\n",
    "    print(x.size())\n",
    "    x = self.stage_4(x)\n",
    "    print(x.size())\n",
    "    x = self.stage_5(x)\n",
    "    print(x.size())\n",
    "    x = self.stage_6(x)\n",
    "    print(x.size())\n",
    "    x = self.stage_7(x)\n",
    "    print(x.size())\n",
    "    x = self.stage_8(x)\n",
    "    print(x.size())\n",
    "    x = self.stage_9(x)\n",
    "    print(x.size())\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "O8XEp4qlrmsn"
   },
   "outputs": [],
   "source": [
    "model = EfficientNet(img_channels = 3, num_classes = 10)\n",
    "x = torch.randn(1, 3, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tGf0Z4FzsnL1",
    "outputId": "7cbdabdc-2d18-4510-9dc9-f90ee82131f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 224, 224])\n",
      "torch.Size([1, 16, 112, 112])\n",
      "torch.Size([1, 24, 112, 112])\n",
      "torch.Size([1, 40, 56, 56])\n",
      "torch.Size([1, 80, 28, 28])\n",
      "torch.Size([1, 112, 14, 14])\n",
      "torch.Size([1, 192, 14, 14])\n",
      "torch.Size([1, 320, 7, 7])\n",
      "torch.Size([1, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0753, -0.6387, -0.1692, -1.2592,  0.0776, -0.8759, -0.8111, -2.0430,\n",
       "         -0.6446, -0.3166]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compound tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkonNgajzNZC"
   },
   "source": [
    "We will compound tune <br>\n",
    "  1) $\\alpha^\\phi$(depth): each of stage layers depth <br>\n",
    "  2) $\\beta^\\phi$(width): num_channels<br>\n",
    "  3) $\\gamma^\\phi$(resolution): image resolution (height * width) <br>\n",
    "  s.t) $\\alpha\\beta^2\\gamma^2$ $\\simeq$ 2 ($\\alpha >=1$, $\\beta>=1$, $\\gamma>=1$) <br>\n",
    "\n",
    "  step: <br>\n",
    "  1) fix $\\phi = 1$. tune $\\alpha$, $\\beta$, $\\gamma$ <br>\n",
    "  2) fix $\\alpha$, $\\beta$, $\\gamma$. tune $\\phi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "QkNtYwUr0qEL"
   },
   "outputs": [],
   "source": [
    "# skip step 1 (I use alpha, beta, gamma written in paper)\n",
    "import numpy as np\n",
    "import math\n",
    "alpha = 1.2\n",
    "beta = 1.1\n",
    "gamma = 1.15\n",
    "phi = [0, 0.5, 1, 2, 3, 4, 5, 6]\n",
    "base_depths = np.array([1, 2, 2, 3, 3, 4, 1]) # exclude stage_1, stage_9\n",
    "base_widths = np.array([32, 16, 24, 40, 80, 112, 192, 320, 1280])\n",
    "depth_tune = {}\n",
    "width_tune = {}\n",
    "resolution  = {}\n",
    "for i in range(len(phi)):\n",
    "  resolution[i] = int(math.ceil(224*(gamma**phi[i])))\n",
    "  depth_tune[i] = np.array(np.ceil(base_depths * (alpha**(phi[i]))), dtype = np.int32)\n",
    "  width_tune[i] =  np.array(np.ceil(base_widths * (beta**(phi[i]))), dtype = np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_NRa7Vp08L5m",
    "outputId": "ee0724d3-2a4a-49ef-f862-8b9b85b54bc3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 224, 1: 241, 2: 258, 3: 297, 4: 341, 5: 392, 6: 451, 7: 519}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IGOEff5v8SeE",
    "outputId": "72b05877-b3f1-4625-92cb-b2af917da60a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: array([1, 2, 2, 3, 3, 4, 1]),\n",
       " 1: array([2, 3, 3, 4, 4, 5, 2]),\n",
       " 2: array([2, 3, 3, 4, 4, 5, 2]),\n",
       " 3: array([2, 3, 3, 5, 5, 6, 2]),\n",
       " 4: array([2, 4, 4, 6, 6, 7, 2]),\n",
       " 5: array([3, 5, 5, 7, 7, 9, 3]),\n",
       " 6: array([ 3,  5,  5,  8,  8, 10,  3]),\n",
       " 7: array([ 3,  6,  6,  9,  9, 12,  3])}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depth_tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h2aA6waB8UB5",
    "outputId": "4698c217-7496-471a-9902-22e901eb36ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: array([  32,   16,   24,   40,   80,  112,  192,  320, 1280]),\n",
       " 1: array([  34,   17,   26,   42,   84,  118,  202,  336, 1343]),\n",
       " 2: array([  36,   18,   27,   44,   88,  124,  212,  352, 1408]),\n",
       " 3: array([  39,   20,   30,   49,   97,  136,  233,  388, 1549]),\n",
       " 4: array([  43,   22,   32,   54,  107,  150,  256,  426, 1704]),\n",
       " 5: array([  47,   24,   36,   59,  118,  164,  282,  469, 1875]),\n",
       " 6: array([  52,   26,   39,   65,  129,  181,  310,  516, 2062]),\n",
       " 7: array([  57,   29,   43,   71,  142,  199,  341,  567, 2268])}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "width_tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "KjxOLAGRitGL"
   },
   "outputs": [],
   "source": [
    "class EfficientNet(nn.Module):\n",
    "  def __init__(self, img_channels, num_classes, width, depth):\n",
    "    '''\n",
    "    inputs:\n",
    "      - img_channels(int)\n",
    "      - num_classes(int)\n",
    "      - width(list): len(width) = 9. It contains each out channels of stage blocks\n",
    "      - depth(list): len(depth) = 7. It contains each number of depth in MBConv blocks\n",
    "    returns:\n",
    "      - EfficientNet\n",
    "    '''\n",
    "\n",
    "    super(EfficientNet, self).__init__()\n",
    "\n",
    "    self.stage_1 = nn.Conv2d(in_channels = img_channels,\n",
    "                          out_channels = width[0],\n",
    "                          kernel_size = (3, 3),\n",
    "                          stride = 1,\n",
    "                          padding = 1)\n",
    "    \n",
    "    self.stage_2 = [MBConv1(in_channels = width[0],\n",
    "                           out_channels = width[1],\n",
    "                           kernel_size = 3,\n",
    "                           stride = 2)]\n",
    "    \n",
    "    for _ in range(depth[0].item() - 1):\n",
    "      self.stage_2.append(MBConv1(in_channels = width[1],\n",
    "                                  out_channels = width[1],\n",
    "                                  kernel_size = 3,\n",
    "                                  stride = 1))\n",
    "    self.stage_2 = nn.Sequential(*self.stage_2)\n",
    "\n",
    "    self.stage_3 = [MBConv6(in_channels = width[1],\n",
    "                           out_channels = width[2],\n",
    "                           kernel_size = 3,\n",
    "                           stride = 1)]\n",
    "    for _ in range(depth[1].item() - 1):\n",
    "      self.stage_3.append(MBConv6(in_channels = width[2],\n",
    "                                  out_channels = width[2],\n",
    "                                  kernel_size = 3,\n",
    "                                  stride = 1))\n",
    "    self.stage_3 = nn.Sequential(*self.stage_3)\n",
    "\n",
    "    self.stage_4 = [MBConv6(in_channels = width[2],\n",
    "                            out_channels = width[3],\n",
    "                            kernel_size = 5,\n",
    "                            stride = 2)]\n",
    "    for _ in range(depth[2].item() - 1):\n",
    "      self.stage_4.append(MBConv6(in_channels = width[3],\n",
    "                            out_channels = width[3],\n",
    "                            kernel_size = 5,\n",
    "                            stride = 1))      \n",
    "    self.stage_4 = nn.Sequential(*self.stage_4)\n",
    "\n",
    "    self.stage_5 = [MBConv6(in_channels = width[3],\n",
    "                            out_channels = width[4],\n",
    "                            kernel_size = 3,\n",
    "                            stride = 2)]\n",
    "    for _ in range(depth[3].item() - 1):\n",
    "      self.stage_5.append(MBConv6(in_channels = width[4],\n",
    "                            out_channels = width[4],\n",
    "                            kernel_size = 3,\n",
    "                            stride = 1))\n",
    "    self.stage_5 = nn.Sequential(*self.stage_5)\n",
    "\n",
    "    self.stage_6 = [MBConv6(in_channels = width[4],\n",
    "                            out_channels = width[5],\n",
    "                            kernel_size = 5,\n",
    "                            stride = 2)]\n",
    "    for _ in range(depth[4].item() - 1):\n",
    "      self.stage_6.append(MBConv6(in_channels = width[5],\n",
    "                            out_channels = width[5],\n",
    "                            kernel_size = 5,\n",
    "                            stride = 1))\n",
    "    self.stage_6 = nn.Sequential(*self.stage_6)\n",
    "\n",
    "    self.stage_7 = [MBConv6(in_channels = width[5],\n",
    "                            out_channels = width[6],\n",
    "                            kernel_size = 5,\n",
    "                            stride = 1)]\n",
    "    for _ in range(depth[5].item() - 1):\n",
    "      self.stage_7.append(MBConv6(in_channels = width[6],\n",
    "                            out_channels = width[6],\n",
    "                            kernel_size = 5,\n",
    "                            stride = 1))\n",
    "    self.stage_7 = nn.Sequential(*self.stage_7)\n",
    "\n",
    "    self.stage_8 = [MBConv6(in_channels = width[6],\n",
    "                           out_channels = width[7],\n",
    "                           kernel_size = 3,\n",
    "                           stride = 2)]\n",
    "    for _ in range(depth[6].item() - 1):\n",
    "      self.stage_8.append(MBConv6(in_channels = width[7],\n",
    "                            out_channels = width[7],\n",
    "                            kernel_size = 3,\n",
    "                            stride = 1))\n",
    "    self.stage_8 = nn.Sequential(*self.stage_8)\n",
    "\n",
    "    self.stage_9 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels = width[7],\n",
    "                  out_channels = width[8],\n",
    "                  kernel_size = (1, 1),\n",
    "                  stride = 1),\n",
    "        nn.AdaptiveMaxPool2d(1),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(in_features = width[8],\n",
    "                  out_features = num_classes)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.stage_1(x)\n",
    "    print(x.size())\n",
    "    x = self.stage_2(x)\n",
    "    print(x.size())\n",
    "    x = self.stage_3(x)\n",
    "    print(x.size())\n",
    "    x = self.stage_4(x)\n",
    "    print(x.size())\n",
    "    x = self.stage_5(x)\n",
    "    print(x.size())\n",
    "    x = self.stage_6(x)\n",
    "    print(x.size())\n",
    "    x = self.stage_7(x)\n",
    "    print(x.size())\n",
    "    x = self.stage_8(x)\n",
    "    print(x.size())\n",
    "    x = self.stage_9(x)\n",
    "    print(x.size())\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "mC5luUBxnupE",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "version = 7\n",
    "my_effnet_b7 = EfficientNet(img_channels = 3,\n",
    "                            num_classes= 10,\n",
    "                            width = width_tune[version],\n",
    "                            depth = depth_tune[version])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, h = resolution[version], resolution[version]\n",
    "x = torch.randn(1, 3, w, h)\n",
    "with torch.no_grad():\n",
    "    out = my_effnet_b7(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oXNnN1GTBc2_"
   },
   "outputs": [],
   "source": [
    "# official PyTorch Efficient Net\n",
    "from torchvision.models import efficientnet\n",
    "b7 = efficientnet.efficientnet_b7(False, False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "EfficientNet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
