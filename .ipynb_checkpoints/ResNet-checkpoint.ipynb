{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my github: https://github.com/withAnewWorld/models_from_scratch\n",
    "# my blog\n",
    "# https://self-deeplearning.blogspot.com/\n",
    "# https://self-deeplearning.tistory.com/\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ref\n",
    "paper: https://arxiv.org/abs/1512.03385?context=cs <br>\n",
    "ref(how to make ResNet with PyTorch, youtuber: Aladdin Persson): <br>\n",
    "https://www.youtube.com/watch?v=DkNIBBBvcPs&list=PLhhyoLH6IjfxeoooqP9rhU3HJIAVAJ3Vz&index=19 <br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 목차\n",
    "1. ResNet 탄생배경\n",
    "2. ResNet architecture\n",
    "3. residual block\n",
    "4. Resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7sNTbncHHajw"
   },
   "source": [
    "## ResNet\n",
    "탄생배경<br>\n",
    ">model layer를 깊게 쌓을수록 성능이 증가, 하지만 gradient vanishing/exploding 문제가 발생하므로 특정 depth 이상 넘어갈수록 model의 성능이 급격히 하락. 이를 해결할 방안 존재? <br>\n",
    "\n",
    "해결 idea <br>\n",
    ">layer를 쌓을 때 적어도 그 전 layer의 성능보다 낮아지지 않도록 하는 방법 고안 <br>\n",
    "\n",
    "방법론 <br>\n",
    "> 새로운 layer를 쌓을 때, 새로운 layer의 output과 이전 layer의 output의 결과값을 더한다. <br>\n",
    "\n",
    "> 근거: 만약 해당 layer의 depth가 최적일 경우 역전파(backpropagation)에 의해 새로운 layer의 output이 0에 수렴할 것이므로 model의 성능이 떨어지지 않음. <br>\n",
    "\n",
    "ref) With the residual learning reformulation, if identity mappings are optimal, the solvers\n",
    "may simply drive the weights of the multiple nonlinear layers toward zero to approach identity mappings. (from paper page 3)\n",
    "\n",
    "```python\n",
    "# pseudo code\n",
    "class ResidualConnection(nn.Module):\n",
    "  def __init__(self):\n",
    "    self.block_1 = nn.Sequential()\n",
    "    self.block_2 = nn.sequential()\n",
    "    ...\n",
    "\n",
    "  def forward(self, x):\n",
    "    identity = x\n",
    "    x = self.block_1(x)\n",
    "    x += identity # 만약 layer가 무리하게 쌓아졌다고 network가 판단할 경우 x -> 0 (by backpropagation)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/withAnewWorld/models_from_scratch/main/pic/residual_learning.png\" height = 250>\n",
    "</p>\n",
    "\n",
    "<div align = 'center'> residual connection(or skip connection) from paper figure 2</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cYaX5pSwrSRJ"
   },
   "source": [
    "## ResNet Architecture\n",
    "ResNet은 VGG Network와 비슷하게 version(layer depth)에 따라 각 block의 depth가 변합니다.  <br>\n",
    "또한 Conv net block 안에서 output size는 변하지 않고 pool을 통해서 size의 크기(H, W)를 각각 반으로 줄이게 됩니다. <br>\n",
    "cf) table 1.에는 각각의 conv block 후에 max pool 표기를 생략되었습니다. <br>(kernel: 2, stride: 2) <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/withAnewWorld/models_from_scratch/main/pic/ResNet_architecture.png\" height = 350>\n",
    "</p>\n",
    "\n",
    "<div align = 'center'> ResNet architecture from paper table 1</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lRR7pLO3shh2"
   },
   "source": [
    "## residual block\n",
    "residual connection을 통해 각 layer의 input과 output을 더한다는 것은 이해가 되지만 실질적인 연산에 대해서는 의문이 드실 것입니다. <br>\n",
    "예를 들어 ResNet(18-layer)의 conv2_x block과 conv3_x block을 살펴보면\n",
    "```python\n",
    "# pseudo code\n",
    "class conv2_x(nn.Module):\n",
    "  def __init__(self):\n",
    "    conv2_x = nn.Sequential(conv(),\n",
    "                          conv())\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''\n",
    "      x(Tensor[N, 64, 56, 56]\n",
    "    '''\n",
    "    identity = x\n",
    "    x = conv2_x(x) # [N, 64, 56, 56]\n",
    "    x += identity \n",
    "  \n",
    "x = nn.MaxPool2d(conv2_x(x) * depth(2)) # [N, 64, 56, 56] -> [N, 64, 28, 28]\n",
    "\n",
    "class conv3_x(nn.Module):\n",
    "  def __init__(self):\n",
    "    conv3_x = nn.Sequential(conv(),\n",
    "                            conv())\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''\n",
    "    conv2_x & max pool의 output(x+identity)이 conv3_x block의 input으로 구성되는 경우라고 생각할 때,\n",
    "    x + identity(Tensor[N, 64, 28, 28])\n",
    "    '''\n",
    "    identity = x\n",
    "    x = conv3_x(x)\n",
    "    x += identity # <- !Error \n",
    "    # x(Tensor[N, 128, 28, 28]), identity(Tensor[N, 64, 28, 28])\n",
    "```\n",
    "즉, 같은 conv block내에서 residual block은 channel이 변하지 않기 때문에 문제가 발생하지 않지만 다음 conv block으로 진입할 때 in channel과 out channel이 다르기 때문에 연산이 되지 않습니다. <br>\n",
    "그러면 이러한 channel 불균형을 어떻게 해결해야 할까요? <br>\n",
    "저자는 크게 2가지 방법을 제시합니다. <br>\n",
    "> 1) zero padding <br>\n",
    "> 2) 1x1 conv 사용 (channel 변환) <br>\n",
    "\n",
    "(ref: paepr 6page, subtitle: Identity vs. Projection Shortcuts.) <br>\n",
    "\n",
    "저희는 2) 1x1 conv을 사용하여 channel 확장을 통해 해당 문제를 해결하여 residual block을 만들어 보겠습니다. <br>\n",
    "\n",
    "```python\n",
    "# pseudo code\n",
    "class ResidualBlock(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(residual_block, self)__init__():\n",
    "    \n",
    "    self.block = nn.Sequential()\n",
    "\n",
    "    self.projection = nn.Conv2d(in_channels = prev layer out channels,\n",
    "                                out_channels = this layer channels,\n",
    "                                kernel_size = (1, 1)\n",
    "                                stride = 1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    identity = x\n",
    "    x = self.block(x)\n",
    "    # 조건문으로 작성해보세요!\n",
    "    try: \n",
    "      x += identity\n",
    "    except:\n",
    "      identity = self.projection(identity)\n",
    "      x += identity\n",
    "\n",
    "    return x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1dRZ5NqM5Q7h"
   },
   "source": [
    "저희는 모든 ResNet model을 설계하진 않고 50, 101, 152 version만 설계하겠습니다. <br>\n",
    "아래 표를 보시면 (50, 101, 152)버전의 모든 conv block은 (1x1, 3x3, 1x1)의 구조로 이루어졌습니다. <br>\n",
    "block 내에서 out_channel, block 갯수만 다르다는 것을 아실 수 있습니다. <br>\n",
    "따라서 residual block을 통해서 (1x1, 3x3, 1x1) conv block을 만들고 반복문을 통해 다시 block을 만들어주면 간단하게 모델을 만들 수 있을 것 같습니다. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/withAnewWorld/models_from_scratch/main/pic/ResNet_architecture.png\" height = 350>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yP8ehR4256xB"
   },
   "source": [
    "```python\n",
    "# pseudo code\n",
    "class ResidualBlock(nn.Module):\n",
    "  def __init__(self):\n",
    "    self.block = nn.Sequential(nn.Conv2d(in_channels,\n",
    "                                        out_channels,\n",
    "                                        kernel_size = (1, 1),\n",
    "                                        stride = 1),\n",
    "                               nn.Conv2d(in_channels,\n",
    "                                        out_channels,\n",
    "                                        kernel_size = (3, 3),\n",
    "                                        stride = 1,\n",
    "                                        padding = 1),\n",
    "                               nn.Conv2d(in_channels,\n",
    "                                        out_channels,\n",
    "                                        kernel_size = (1, 1),\n",
    "                                        stride = 1))\n",
    "    self.projection\n",
    "    \n",
    "  def forward(self, x):\n",
    "    return residual connection\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    # version check\n",
    "\n",
    "    self.conv1\n",
    "    self.conv2 = [ResidualBlock]\n",
    "    for _ in range(num_repeats of conv2 block - 1):\n",
    "      self.conv2.append(ResidualBlock)\n",
    "    self.conv2.append(nn.MaxPool2d())\n",
    "    self.conv2 = nn.Sequential(*self.conv2)\n",
    "\n",
    "    ...\n",
    "\n",
    "    self.classifier = nn.Sequential(nn.AvgPool2d(),\n",
    "                                    nn.Flatten(),\n",
    "                                    Linear(out_features = num_classes),\n",
    "                                    nn.Softmax() # 본문에서는 생략했습니다.)\n",
    "  def forward(self, x)\n",
    "    pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ew6cmu4u63a1"
   },
   "source": [
    "## Let's make ResNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "y_NX6dCqXgoe"
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "  def __init__(self, in_channels, mid_channels):\n",
    "    super(ResidualBlock, self).__init__()\n",
    "    self.block = nn.Sequential(\n",
    "        nn.Conv2d(in_channels = in_channels,\n",
    "                  out_channels = mid_channels,\n",
    "                  kernel_size = (1, 1),\n",
    "                  stride = 1\n",
    "                  ),\n",
    "        nn.Conv2d(in_channels = mid_channels,\n",
    "                  out_channels = mid_channels,\n",
    "                  kernel_size = (3, 3),\n",
    "                  stride = 1,\n",
    "                  padding = 1),\n",
    "        nn.Conv2d(in_channels = mid_channels,\n",
    "                  out_channels = mid_channels*4,\n",
    "                  kernel_size = (1, 1),\n",
    "                  stride = 1,\n",
    "                  )\n",
    "    )\n",
    "    self.projection = nn.Conv2d(in_channels = in_channels,\n",
    "                                 out_channels = mid_channels*4,\n",
    "                                 kernel_size = (1, 1),\n",
    "                                 stride = 1)\n",
    "  def forward(self, x):\n",
    "    identity = x\n",
    "    x = self.block(x)\n",
    "    try:\n",
    "      x += identity\n",
    "    except:\n",
    "      identity = self.projection(identity)\n",
    "      x += identity\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "7RTTE2A8XWFg"
   },
   "outputs": [],
   "source": [
    "# cf)  batcch norm, activation fn, ... 생략\n",
    "class ResNet(nn.Module):\n",
    "  def __init__(self, version, img_channels = 3, num_classes = 1000):\n",
    "    super(ResNet, self).__init__()\n",
    "    '''\n",
    "    inputs:\n",
    "      - version(int): one of (50, 101, 152)\n",
    "      - img_channels(int)\n",
    "      - num_classes(int)\n",
    "    returns:\n",
    "      - logits(tensor[N, num_classes])\n",
    "    '''\n",
    "    if version == 50:\n",
    "      num_repeats = [3, 4, 6, 3]\n",
    "    elif version == 101:\n",
    "      num_repeats = [3, 4, 23, 3]\n",
    "    elif version == 152:\n",
    "      num_repeats = [3, 8, 36, 3]\n",
    "    else:\n",
    "      raise ValueError('Please check the version(one of (50, 101, 152))')\n",
    "\n",
    "    self.conv1 = nn.Conv2d(in_channels= img_channels ,\n",
    "                          out_channels = 64,\n",
    "                          kernel_size = (7, 7),\n",
    "                          stride = 2,\n",
    "                          padding = 3)\n",
    "    self.max_pool = nn.MaxPool2d(kernel_size = (3, 3),\n",
    "                              stride = 2,\n",
    "                              padding = 1)\n",
    "    \n",
    "    self.conv2_x = self._make_block(64, 64, num_repeats[0])\n",
    "    self.conv3_x = self._make_block(256, 128, num_repeats[1])  \n",
    "    self.conv4_x = self._make_block(512, 256, num_repeats[2])\n",
    "    self.conv5_x = self._make_block(1024, 512, num_repeats[3])\n",
    "    \n",
    "    self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "    self.flatten = nn.Flatten()\n",
    "    self.fc = nn.Linear(in_features = 2048*1*1,\n",
    "                        out_features = num_classes)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.max_pool(self.conv1(x))\n",
    "    x = self.conv2_x(x)\n",
    "    x = self.conv3_x(x)\n",
    "    x = self.conv4_x(x)\n",
    "    x = self.conv5_x(x)\n",
    "    x = self.avg_pool(x)\n",
    "    x = self.fc(self.flatten(x))\n",
    "    return x\n",
    "\n",
    "  def _make_block(self, in_channels, mid_channels, num_repeats):\n",
    "    '''\n",
    "    out channels of Sequential blocks in ResNet go like this\n",
    "    block_1: in_channels -> mid_channels / mid_channels -> mid_channels -> / mid_channels -> mid_channels * 4  \n",
    "    block_2: mid_channels * 4 -> mid_channels / mid_channels -> mid_channels / mid_channels -> mid_channels * 4\n",
    "    block_3: mid_channels * 4 -> mid_channels / mid_channels -> mid_channels / mid_channels -> mid_channels * 4\n",
    "    ...\n",
    "\n",
    "    inputs\n",
    "      - in_channels(int): in_channels of first blocks's first conv\n",
    "      - mid_channels(int): out channels of block's first two conv (ex. conv_2_x: 64)\n",
    "      - num_repeats(int): num of blocks\n",
    "    returns\n",
    "      - Sequential block\n",
    "    '''\n",
    "    layers = []\n",
    "    layers.append(ResidualBlock(in_channels, mid_channels))\n",
    "    in_channels = mid_channels * 4\n",
    "    for _ in range(num_repeats - 1):\n",
    "      layers.append(ResidualBlock(in_channels, mid_channels))\n",
    "    if mid_channels != 512: \n",
    "      layers.append(nn.MaxPool2d(kernel_size = (2, 2),\n",
    "                                  stride = 2))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ovkRm3I_cDx9"
   },
   "outputs": [],
   "source": [
    "model = ResNet(152, 3, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "r9oqMk9Qlaud"
   },
   "outputs": [],
   "source": [
    "output = model(torch.randn(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pnGLn2rbleVi",
    "outputId": "f3016906-ada6-4759-e1ac-c48cf1df28f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ResNet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
