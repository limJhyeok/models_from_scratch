{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my github: https://github.com/withAnewWorld/models_from_scratch\n",
    "# my blog\n",
    "#  https://self-deeplearning.blogspot.com/\n",
    "#  https://self-deeplearning.tistory.com/\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FOy804mSoWGZ"
   },
   "source": [
    "## Ref\n",
    "1. LeNet <br>\n",
    "paper: http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf <br>\n",
    "Youtube(Aladdin Persson):  \n",
    "https://www.youtube.com/watch?v=fcOW-Zyb5Bo&list=PLhhyoLH6IjfxeoooqP9rhU3HJIAVAJ3Vz&index=16 <br>\n",
    "\n",
    "2. CNN<br>\n",
    " https://cs231n.github.io/convolutional-networks/ <br>\n",
    "3. Linear Classification<br>\n",
    " https://cs231n.github.io/linear-classify/ <br>\n",
    " \n",
    "## 목차\n",
    "1. LeNet architecture <br>\n",
    "2. torch.nn <br>\n",
    "3. torch.nn.Module <br>\n",
    "4. torch.nn.Sequential <br>\n",
    "5. nn.Seuqntial in nn.Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/withAnewWorld/models_from_scratch/main/pic/LeNet.png\" height = 500>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVeIYkKsupOi"
   },
   "source": [
    "PyTorch model API <br>\n",
    "> 1) torch.nn <br>\n",
    "> 2) torch.nn.Module <br>\n",
    "> 3) torch.nn.Sequential <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "khrTRSOUqC7E"
   },
   "source": [
    "\n",
    "```python\n",
    "# pseudo code\n",
    "def LeNet(image, num_classes):\n",
    "  '''\n",
    "  image classification task를 진행한다고 가정.\n",
    "  이미지 Tensor를 LeNet에 feed할 경우, 해당 이미지의 각 class의 logits(예측값)을 반환. \n",
    "  이 중 logit값이 가장 큰 값이 해당 network가 예측하는 class.\n",
    "  img의 크기(height, width)는 위의 예와 같이 32 x 32라고 가정.\n",
    "  inputs:\n",
    "    - image(Tensor[N, C, H, W])\n",
    "    - num_classes(int)\n",
    "  returns:\n",
    "    - logits(Tensor[N, num_classes])\n",
    "  '''\n",
    "```\n",
    "  convolution network를 거친 후 image의 크기는 다음과 같이 변하게 됩니다. <br>\n",
    "  W = 1 + (W - kernel_size + 2*padding)/(stride) (no dilation conv) <br>\n",
    "  H = 1 + (H - kernel_size + 2*padding)/(stride) (no dilation conv) <br>\n",
    "  논문 저자는 모든 conv net의 kernel_size를 5x5로 고정했습니다. <br>\n",
    "  따라서, C1을 거친후 32x32의 image tensor가 28x28로 바뀌기 위해서 stride = 2 <br>(모든 conv net padding = 0으로 고정) <br>\n",
    "  이같은 방식으로 모든 conv net의 kernel_size, stride, padding을 설정할 수 있습니다. <br>\n",
    "  sub-sampling의 경우 average pool을 사용했습니다. <br>\n",
    "```python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dp5hFhZKvC36"
   },
   "source": [
    "## torch.nn\n",
    "torch.nn에 존재하는 neural network 함수를 하나씩 이용해서 model을 설계하는 방법."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "002XM2dXvPeO"
   },
   "outputs": [],
   "source": [
    "def LeNet(image, num_classes):\n",
    "    '''\n",
    "    inputs:\n",
    "        - image(Tensor[N, C, H, W]): (N: num_images, C: num_channels, H: image Height, W: image Width)\n",
    "        - num_classes(int)\n",
    "    returns:\n",
    "        - x(Tensor[N, num_classes])\n",
    "    '''\n",
    "    C1 = nn.Conv2d(in_channels = 1,\n",
    "               out_channels = 6,\n",
    "               kernel_size = (5, 5),\n",
    "               stride= (1, 1),\n",
    "               padding = (0, 0))\n",
    "\n",
    "    S2 = nn.AvgPool2d(kernel_size = (2,2),\n",
    "                             stride = (2,2))\n",
    "\n",
    "    C3 = nn.Conv2d(in_channels = 6,\n",
    "               out_channels = 16,\n",
    "               kernel_size = (5, 5),\n",
    "               stride= (1, 1),\n",
    "               padding = (0, 0))\n",
    "\n",
    "    S4 = nn.AvgPool2d(kernel_size = (2,2),\n",
    "                             stride = (2,2))\n",
    "\n",
    "    C5 = nn.Conv2d(in_channels = 16,\n",
    "               out_channels = 120,\n",
    "               kernel_size = (5, 5),\n",
    "               stride= (1, 1),\n",
    "               padding = (0, 0))\n",
    "\n",
    "    F6 = nn.Linear(in_features = 120, out_features = 84)\n",
    "\n",
    "    flatten = nn.Flatten()\n",
    "\n",
    "    classifier = nn.Linear(in_features = 84, out_features = num_classes)\n",
    "\n",
    "    x = C1(image)\n",
    "    print(x.size())\n",
    "    x = S2(x)\n",
    "    print(x.size())\n",
    "    x = C3(x)\n",
    "    print(x.size())\n",
    "    x = S4(x)\n",
    "    print(x.size())\n",
    "    x = C5(x)\n",
    "    print(x.size())\n",
    "    x = flatten(x)\n",
    "    print(x.size())\n",
    "    x = F6(x)\n",
    "    print(x.size())\n",
    "\n",
    "    x = classifier(x)\n",
    "    print(x.size())\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rGQJ4tkEwnvL",
    "outputId": "a7b84daf-13ea-45de-f268-c83d772bdc13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 6, 28, 28])\n",
      "torch.Size([64, 6, 14, 14])\n",
      "torch.Size([64, 16, 10, 10])\n",
      "torch.Size([64, 16, 5, 5])\n",
      "torch.Size([64, 120, 1, 1])\n",
      "torch.Size([64, 120])\n",
      "torch.Size([64, 84])\n",
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42) # deterministic\n",
    "image = torch.randn(64, 1, 32, 32)\n",
    "num_classes = 10\n",
    "output = LeNet(image, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9dQ4zCTUxDSS",
    "outputId": "3947c9e3-6405-4fe1-bede-8359b7bfb853",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0533, -0.0760,  0.0290, -0.0205,  0.0147, -0.0804, -0.0519, -0.1315,\n",
       "        -0.1492,  0.0654], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjctcrC0xSrC"
   },
   "source": [
    "## nn.Module\n",
    "위와 같이 torch.nn에 존재하는 단순히 함수를 나열하여 개발을 할 경우 여러가지 문제가 발생합니다. <br>\n",
    "> 1) 모델의 크기가 커질수록 코드 가독성 현저하게 떨어짐 <br>\n",
    "  -> code refactoring 비용 증가<br>\n",
    "> 2) 현재 많은 model이 module화 되어 있기 때문에 특정 layers(block)을 교체하거나 수정하는 경우가 빈번합니다. 하지만 위의 방법대로는 매우 어렵습니다.<br>\n",
    "> 3) torch에서 제공하는 많은 API를 사용하기 어려워짐 <br>\n",
    "see utils in nn.Module<br>\n",
    " https://pytorch.org/docs/stable/generated/torch.nn.Module.html <br>\n",
    "-> 해당 문제점은 모두 객체지향적으로 코드가 작성되어 있지 않기 때문에 발생하는 문제입니다. <br>\n",
    "-> 모델 그리고 모델을 이루는 모듈 또한 하나의 객체로 바라볼 수 있으므로 객체지향적으로 코드를 작성하는 것이 일반적으로 적합합니다.\n",
    "<br>\n",
    "\n",
    "PyTorch를 사용하는 많은 개발자는 nn.Module을 이용하여 DL model 개발 중입니다.<br>\n",
    "```python\n",
    "class Net(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Net, self).__init__() <- nn.Module의 init함수 상속\n",
    "\n",
    "  def forward(self, x):\n",
    "```\n",
    "PyTorch는 model 설계에 필요한 까다로운 함수들을 모두 nn.Module Class에 구현해 놓았습니다. <br>\n",
    "이에 따라 개발자는 model을 설계할 때 nn.Module을 상속받음으로써 까다로운 부분에 대해 신경쓰지 않고 <br>\n",
    "1) model의 network를 초기화할 init 함수 <br>\n",
    "2) input을 model에 feed하여 결과값을 산출하는 forward 함수<br>\n",
    "<br>\n",
    "만을 고려하면 손쉽게 model을 만들 수 있습니다. <br>\n",
    "주의해야할 점은 model에 input을 feed할 때 forward함수를 직접 호출하는 것이 아닌 model을 초기화 하고 변수로 설정하셔야 합니다. <br>\n",
    "```python\n",
    "model = Net()\n",
    "out = model.forward(x) # <- don't do ilke this!\n",
    "out = model(x) # <- it's correct\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "sqgm84_un3rG"
   },
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(LeNet, self).__init__()\n",
    "    self.relu = nn.ReLU()\n",
    "    self.pool = nn.AvgPool2d(kernel_size = (2,2),\n",
    "                             stride = (2,2))\n",
    "    self.conv1 = nn.Conv2d(in_channels = 1,\n",
    "                           out_channels = 6,\n",
    "                           kernel_size = (5, 5),\n",
    "                           stride= (1, 1),\n",
    "                           padding = (0, 0))\n",
    "    self.conv2 = nn.Conv2d(in_channels = 6,\n",
    "                           out_channels = 16,\n",
    "                           kernel_size = (5, 5),\n",
    "                           stride= (1, 1),\n",
    "                           padding = (0, 0))\n",
    "    self.conv3 = nn.Conv2d(in_channels = 16,\n",
    "                           out_channels = 120,\n",
    "                           kernel_size = (5, 5),\n",
    "                           stride= (1, 1),\n",
    "                           padding = (0, 0))\n",
    "    self.linear1 = nn.Linear(120, 84)\n",
    "    self.linear2 = nn.Linear(84, 10)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.relu(self.conv1(x))\n",
    "    x = self.pool(x)\n",
    "    x = self.relu(self.conv2(x))\n",
    "    x = self.pool(x)\n",
    "    x = self.relu(self.conv3(x)) # Nx120x1x1 -> Nx120\n",
    "    x = x.reshape(x.shape[0], -1)\n",
    "    x = self.relu(self.linear1(x))\n",
    "    return self.linear2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "PUcr3U66pj1e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = torch.randn((64, 1, 32, 32))\n",
    "model = LeNet()\n",
    "with torch.no_grad():\n",
    "  out = model(image)\n",
    "out.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLufxvXO0fvb"
   },
   "source": [
    "## nn.Sequential\n",
    "일반적으로 nn.Module의 경우 전체 모델을 설계할 때 나타냅니다. <br>\n",
    "하지만 model의 일부 구조가 반복되는 경우 코드 가독성 및 유지보수를 위해 이를 하나의 객체로 묶고 싶어집니다. <br>\n",
    "이러한 구조를 원할 때 유용하게 nn.Sequential을 이용하면 모델 설계에 도움이 됩니다.<br>\n",
    "<br>\n",
    "cf) nn.Seuqntial은 layer을 순서대로 설정하고 input을 feed하면 순서에 따라 layer를 적용합니다. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nkwkcg8S1PoV",
    "outputId": "65a819b8-ccb1-4c00-c41c-c68cda5ddb9c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(in_channels = 1,\n",
    "                           out_channels = 6,\n",
    "                           kernel_size = (5, 5),\n",
    "                           stride= (1, 1),\n",
    "                           padding = (0, 0)),\n",
    "                     \n",
    "    nn.ReLU(),\n",
    "\n",
    "    nn.AvgPool2d(kernel_size = (2,2),\n",
    "                             stride = (2,2)),\n",
    "                     \n",
    "    nn.Conv2d(in_channels = 6,\n",
    "                           out_channels = 16,\n",
    "                           kernel_size = (5, 5),\n",
    "                           stride= (1, 1),\n",
    "                           padding = (0, 0)),\n",
    "    \n",
    "    nn.ReLU(),\n",
    "\n",
    "    nn.AvgPool2d(kernel_size = (2,2),\n",
    "                                stride = (2,2)),    \n",
    "    \n",
    "    nn.Conv2d(in_channels = 16,\n",
    "                              out_channels = 120,\n",
    "                              kernel_size = (5, 5),\n",
    "                              stride= (1, 1),\n",
    "                              padding = (0, 0)),\n",
    "    \n",
    "    nn.ReLU(),\n",
    "\n",
    "    nn.Flatten(),\n",
    "    \n",
    "    nn.Linear(120, 84),\n",
    "\n",
    "    nn.ReLU(),\n",
    "\n",
    "    nn.Linear(84, 10)\n",
    ")\n",
    "image= torch.randn(64, 1, 32, 32)\n",
    "output = model(image)\n",
    "output.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Sequntial in nn.Moduel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(LeNet, self).__init__()\n",
    "    \n",
    "    self.c1_s2_block = nn.Sequential(\n",
    "        nn.Conv2d(in_channels = 1,\n",
    "                   out_channels = 6,\n",
    "                   kernel_size = (5, 5),\n",
    "                   stride= (1, 1),\n",
    "                   padding = (0, 0)),\n",
    "\n",
    "        nn.ReLU(),\n",
    "\n",
    "        nn.AvgPool2d(kernel_size = (2,2),\n",
    "                     stride = (2,2))\n",
    "    )\n",
    "    \n",
    "    self.c3_s4_block = nn.Sequential(\n",
    "        nn.Conv2d(in_channels = 6,\n",
    "                   out_channels = 16,\n",
    "                   kernel_size = (5, 5),\n",
    "                   stride= (1, 1),\n",
    "                   padding = (0, 0)),\n",
    "        nn.ReLU(),\n",
    "\n",
    "        nn.AvgPool2d(kernel_size = (2,2),\n",
    "                    stride = (2,2))\n",
    "    )\n",
    "    \n",
    "    self.c5_f6_block = nn.Sequential(\n",
    "        nn.Conv2d(in_channels = 16,\n",
    "                  out_channels = 120,\n",
    "                  kernel_size = (5, 5),\n",
    "                  stride= (1, 1),\n",
    "                  padding = (0, 0)),\n",
    "    \n",
    "        nn.ReLU(),\n",
    "\n",
    "        nn.Flatten(),\n",
    "\n",
    "        nn.Linear(120, 84),\n",
    "        \n",
    "        nn.ReLU(),\n",
    "    )    \n",
    "        \n",
    "    self.classifier = nn.Linear(84, 10)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.c1_s2_block(x)\n",
    "    x = self.c3_s4_block(x)\n",
    "    x = self.c5_f6_block(x)\n",
    "    x = self.classifier(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x= torch.randn(64, 1, 32, 32)\n",
    "model = LeNet()\n",
    "with torch.no_grad():\n",
    "    output = model(x)\n",
    "output.size()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LeNet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "811efbe449f8030a1357d21838ca884c523c3e7a6d4d3bfbec51bb5a60890126"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
