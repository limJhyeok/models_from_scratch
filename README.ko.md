# PyTorchë¡œ ë”¥ëŸ¬ë‹ ëª¨ë¸ ì§ì ‘ êµ¬í˜„í•˜ê¸°

**ì–¸ì–´ ì„ íƒ / Language Selection:**

- [ğŸ‡°ğŸ‡· í•œêµ­ì–´ (Korean)](README.ko.md)
- [ğŸ‡ºğŸ‡¸ English](README.md)

ì´ ë¦¬í¬ì§€í† ë¦¬ëŠ” **PyTorch**ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘í•œ ë”¥ëŸ¬ë‹ì˜ ê¸°ì´ˆ ëª¨ë¸ì„ ì§ì ‘ êµ¬í˜„í•œ ë‚´ìš©ì„ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ í”„ë¡œì íŠ¸ì˜ ëª©ì ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

1. **PyTorch ìˆ™ë‹¬**: PyTorchë¥¼ í™œìš©í•˜ì—¬ ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ì²˜ìŒë¶€í„° êµ¬í˜„í•˜ë©° ê¸°ìˆ ì„ ì—°ë§ˆí•©ë‹ˆë‹¤.
2. **í•µì‹¬ ì•„í‚¤í…ì²˜ ì´í•´**: í˜„ëŒ€ ë”¥ëŸ¬ë‹ì˜ ê¸°ì´ˆë¥¼ ì´ë£¨ëŠ” CNN(Convolutional Neural Network)ê³¼ Transformer ì•„í‚¤í…ì²˜ì— ëŒ€í•œ ì‹¬ë„ ìˆëŠ” ì´í•´ë¥¼ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.

## êµ¬í˜„ëœ ëª¨ë¸

<table>
    <tr>
        <th>Model</th>
        <th>Colab</th>
    </tr>
    <tr>
        <td><strong>LeNet</strong><br>One of the earliest convolutional neural networks introduced by Yann LeCun. 
          A foundational model for image classification tasks.</td>
        <td><a href="https://colab.research.google.com/github/limJhyeok/models_from_scratch/blob/main/LeNet.ipynb"><img
                    src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab"></a></td>
    </tr>
    <tr>
        <td><strong>VGG Network</strong><br>Known for its simplicity and use of very deep networks composed of 3x3
            convolution layers. Explores the role of network depth in improving accuracy.</td>
        <td><a href="https://colab.research.google.com/github/limJhyeok/models_from_scratch/blob/main/VGG.ipynb"><img
                    src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab"></a></td>
    </tr>
    <tr>
        <td><strong>GoogLeNet (Inception)</strong><br>Introduced the <em>Inception module</em>, allowing for efficient
            computation with reduced parameters. Demonstrates the importance of designing models with computational
            efficiency in mind.</td>
        <td><a href="https://colab.research.google.com/github/limJhyeok/models_from_scratch/blob/main/GoogLeNet.ipynb"><img
                    src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab"></a></td>
    </tr>
    <tr>
        <td><strong>ResNet</strong><br>Introduced the concept of <em>residual connections</em>, addressing the vanishing
            gradient problem in deep networks. Paved the way for very deep architectures with hundreds of layers.</td>
        <td><a href="https://colab.research.google.com/github/limJhyeok/models_from_scratch/blob/main/ResNet.ipynb"><img
                    src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab"></a></td>
    </tr>
    <tr>
        <td><strong>EfficientNet</strong><br>Scales models effectively across width, depth, and resolution using a
            compound scaling method. Combines efficiency and performance, achieving state-of-the-art results.</td>
        <td><a
                href="https://colab.research.google.com/github/limJhyeok/models_from_scratch/blob/main/EfficientNet.ipynb"><img
                    src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab"></a></td>
    </tr>
    <tr>
        <td><strong>Transformer</strong><br>Revolutionized deep learning by replacing RNNs with self-attention
            mechanisms. Forms the backbone of many modern models like BERT and GPT.</td>
        <td><a
                href="https://colab.research.google.com/github/limJhyeok/models_from_scratch/blob/main/Transformer.ipynb"><img
                    src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab"></a></td>
    </tr>
</table>

## ëª¨ë¸ ì„ ì • ì´ìœ 
ì´ ëª¨ë¸ë“¤ì€ ë”¥ëŸ¬ë‹ ì•„í‚¤í…ì²˜ì˜ ë°œì „ì„ ì‚´í´ë³´ê¸°ì— ì í•©í•©ë‹ˆë‹¤:

- **LeNet**: CNNì˜ ê¸°ë³¸ ê°œë…ì„ ì†Œê°œí•˜ë©° í˜„ëŒ€ CNN ê¸°ë°˜ ëª¨ë¸ì˜ ì‹œì´ˆê°€ ë˜ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤.  
- **VGG**ì™€ **GoogLeNet**: ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ê¹Šê²Œ ìŒ“ì•˜ì„ ë•Œ ì„±ëŠ¥ì´ í–¥ìƒëœë‹¤ëŠ” ì§ê´€ì„ ì‹¤í—˜ì ìœ¼ë¡œ ì¦ëª…í–ˆìŠµë‹ˆë‹¤.  
- **ResNet**: ë„¤íŠ¸ì›Œí¬ ê¹Šì´ê°€ ê¸¸ì–´ì§ì— ë”°ë¥¸ í•™ìŠµ ë¬¸ì œ(Vanishing gradient)ë¥¼ í•´ê²°í•©ë‹ˆë‹¤.
- **EfficientNet**: íš¨ìœ¨ì ì¸ ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ íƒêµ¬í•©ë‹ˆë‹¤.  
- **Transformer**: ì „í†µì ì¸ CNNê³¼ RNNì˜ í•œê³„ë¥¼ ë„˜ì–´ ì–´í…ì…˜(attention) ë©”ì»¤ë‹ˆì¦˜ì˜ ê°€ëŠ¥ì„±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.  

## ì°¸ê³  ìë£Œ

- PyTorch ê³µì‹ ë¬¸ì„œ: [PyTorch Documentation](https://pytorch.org/docs/)
- ì›ë³¸ ë…¼ë¬¸:
    - [LeNet](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf)
    - [VGG](https://arxiv.org/abs/1409.1556)
    - [GoogLeNet](https://arxiv.org/abs/1409.4842)
    - [ResNet](https://arxiv.org/abs/1512.03385)
    - [EfficientNet](https://arxiv.org/abs/1905.11946)
    - [Transformer](https://arxiv.org/abs/1706.03762)
