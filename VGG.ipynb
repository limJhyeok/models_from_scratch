{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my github: https://github.com/withAnewWorld/models_from_scratch\n",
    "# my blog\n",
    "# https://self-deeplearning.blogspot.com/\n",
    "# https://self-deeplearning.tistory.com/\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ref\n",
    "paper: https://arxiv.org/pdf/1409.1556.pdf <br>\n",
    "youtube(Aladdin Persson): <br>\n",
    " https://www.youtube.com/watch?v=ACmuBbuXn20&list=PLhhyoLH6IjfxeoooqP9rhU3HJIAVAJ3Vz&index=17 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 목차\n",
    "1. VGG Network 의의\n",
    "2. VGG Network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KgCBFhVV89Kh"
   },
   "source": [
    "## VGG Network\n",
    "의의 <br>\n",
    "> 1) Alex Net(conv net기반 model의 전성기를 열은 network)이후 conv net기반 model의 layer를 깊게 쌓을수록 성능이 좋아질 것이라는 예측을 실험적으로 보임 <br>\n",
    "> 2) 작은 kernel_size (3x3)를 이용하여 model의 성능을 끌어올림 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 845
    },
    "id": "INtr9zDV-7Ae",
    "outputId": "5308b690-23a0-4be6-db3f-e6965f9e2855"
   },
   "source": [
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/withAnewWorld/models_from_scratch/main/pic/VGG.png\" height = 500>\n",
    "</p>\n",
    "<p align= \"center\">VGG architecture from VGG paper table 1 </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_UrCvr9l_WMZ"
   },
   "source": [
    "VGG net의 경우 여러 layer의 갯수에 따라 version이 있습니다. (11, 13, 16, 19) <br>\n",
    "모든 conv net은 kernel_size: 3x3, stride: 1, padding: 1로 고정입니다. <br>\n",
    "(-> Height, Width 고정) <br>\n",
    "다만, Max Pool layer를 거칠 때마다 Height와 Width가 각각 반으로 줄어듭니다.<br> (kernel_size: 2, stride: 2) <br>\n",
    "table 1.을 보시면 저자가 왼쪽에서부터 오른쪽으로 새로운 layer가 추가(or 변경)되는 것에 대해 굵은 글씨체로 써줬습니다. <br>\n",
    "따라서 version이 변경될 때마다 굵은 글씨의 layer를 변경하면 될 것 같습니다. <br>\n",
    "cf) VGG16의 conv1(kernel_size: 1)의 경우 model 비교를 위해 저자가 설정한 것으로 생략하겠습니다. <br>\n",
    "in spite of the same depth, the configuration C (which\n",
    "contains three 1 × 1 conv. layers), performs worse than the configuration D, (from paper 6page)\n",
    "```python\n",
    "# pseudo code\n",
    "class VGG(version, ...):\n",
    "  1) VGG_11을 설계한다.\n",
    "  2) if version > 11:\n",
    "    layer 추가\n",
    "  3) if version > 13:\n",
    "    layer 추가\n",
    "  4) if version > 16:\n",
    "    layer 추가\n",
    "  5) if version == 19:\n",
    "    layer 추가\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''\n",
    "    input:\n",
    "      - x(Tensor[N, C, 224, 224])\n",
    "    output:\n",
    "      - logits(Tensor[N, num_classes])\n",
    "    '''\n",
    "\n",
    "```\n",
    "그러면 VGG 11을 pseudo code로 설계해보겠습니다.\n",
    "```python\n",
    "# pseudo code\n",
    "class VGG(nn.Module):\n",
    "  def __init__(self, version, ...):\n",
    "    super(VGG, self).__init__()\n",
    "    self.block_1 = nn.Conv2d() C(3->64), depth: 1\n",
    "    self.pool = nn.MaxPool2d() (Height, Width: 224 -> 112)\n",
    "\n",
    "    self.block_2 = nn.Conv2d() C(64->128), depth: 1\n",
    "    self.pool = MaxPool2d() (Height, Width: 112 -> 56)\n",
    "\n",
    "    self.block_3 = nn.Conv2d() C(128->256) depth: 2\n",
    "    self.pool = MaxPool2d() (Height, Width: 56 -> 28)\n",
    "\n",
    "    self.block_4 = nn.Conv2d() C(256->512) depth: 2\n",
    "    self.pool = MaxPool2d() (Height, Width: 28 -> 14)\n",
    "\n",
    "    self.block_5 = nn.Conv2d() C(512->512) depth: 2\n",
    "    self.pool = MaxPool2d() (Height, Width: 14 -> 7)\n",
    "\n",
    "    self.linear_block = nn.Sequential(\n",
    "      nn.Flatten(),\n",
    "      nn.Linear(in_features = 512*7*7(C*H*W), out_featuers = 4096),\n",
    "      nn.Linear(in_features = 4096, out_featuers = 4096),\n",
    "      nn.Linear(in_features = 4096, out_featuers = num_classes(1000)),\n",
    "      nn.Softmax(dim = -1)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    pass\n",
    "\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ypGdcxMOoStL"
   },
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "  def __init__(self, version, in_channels, H, W):\n",
    "    '''\n",
    "    inputs:\n",
    "      - verison(int): one of (11, 13, 16, 19)\n",
    "      - in_channels: img channels (ex. RGB: 3)\n",
    "      - H: img height\n",
    "      - W: img width\n",
    "    '''\n",
    "    \n",
    "    super().__init__()\n",
    "    num_pool = 5\n",
    "    self.block_1 = nn.ModuleList([\n",
    "      nn.Conv2d(in_channels = in_channels, \n",
    "                            out_channels = 64, \n",
    "                            kernel_size = (3, 3),\n",
    "                            stride = 1,\n",
    "                            padding = 1),\n",
    "     nn.MaxPool2d(kernel_size = (2, 2),\n",
    "        stride = 2)                               \n",
    "    ])\n",
    "    if version > 11:\n",
    "      self.block_1.insert(1, nn.Conv2d(in_channels = 64, \n",
    "                            out_channels = 64, \n",
    "                            kernel_size = (3, 3),\n",
    "                            stride = 1,\n",
    "                            padding = 1))\n",
    "\n",
    "    self.block_2 = nn.ModuleList([\n",
    "        nn.Conv2d(in_channels = 64,\n",
    "                            out_channels = 128,\n",
    "                            kernel_size = (3,3),\n",
    "                            stride = 1,\n",
    "                            padding = 1),\n",
    "        nn.MaxPool2d(kernel_size = (2, 2),\n",
    "              stride = 2)\n",
    "    ])\n",
    "    if version > 11:\n",
    "      self.block_2.insert(1, nn.Conv2d(in_channels = 128,\n",
    "                            out_channels = 128,\n",
    "                            kernel_size = (3,3),\n",
    "                            stride = 1,\n",
    "                            padding = 1))\n",
    "    \n",
    "    self.block_3 = nn.ModuleList([\n",
    "        nn.Conv2d(in_channels = 128,\n",
    "                              out_channels = 256,\n",
    "                            kernel_size = (3,3),\n",
    "                            stride = 1,\n",
    "                            padding = 1),\n",
    "        nn.Conv2d(in_channels = 256,\n",
    "                              out_channels = 256,\n",
    "                              kernel_size = (3, 3),\n",
    "                              stride = 1,\n",
    "                              padding = 1),\n",
    "        nn.MaxPool2d(kernel_size = (2, 2),\n",
    "                               stride = 2)\n",
    "    ])\n",
    "    if version > 13:\n",
    "      self.block_3.insert(2, nn.Conv2d(in_channels = 256,\n",
    "                              out_channels = 256,\n",
    "                              kernel_size = (3, 3),\n",
    "                              stride = 1,\n",
    "                              padding = 1))\n",
    "    if version > 16:\n",
    "      self.block_3.insert(3, nn.Conv2d(in_channels = 256,\n",
    "                              out_channels = 256,\n",
    "                              kernel_size = (3, 3),\n",
    "                              stride = 1,\n",
    "                              padding = 1))\n",
    "\n",
    "\n",
    "    self.block_4 = nn.ModuleList([\n",
    "        nn.Conv2d(in_channels = 256,\n",
    "                            out_channels = 512,\n",
    "                            kernel_size = (3,3),\n",
    "                            stride = 1,\n",
    "                            padding = 1),\n",
    "        nn.Conv2d(in_channels = 512,\n",
    "                                out_channels = 512,\n",
    "                                kernel_size = (3,3),\n",
    "                                stride = 1,\n",
    "                                padding = 1),\n",
    "        nn.MaxPool2d(kernel_size = (2, 2),\n",
    "                               stride = 2)\n",
    "    ])\n",
    "    if version > 13:\n",
    "      self.block_4.insert(2, nn.Conv2d(in_channels = 512,\n",
    "                                out_channels = 512,\n",
    "                                kernel_size = (3,3),\n",
    "                                stride = 1,\n",
    "                                padding = 1))\n",
    "    if version > 16:\n",
    "      self.block_4.insert(3, nn.Conv2d(in_channels = 512,\n",
    "                                out_channels = 512,\n",
    "                                kernel_size = (3,3),\n",
    "                                stride = 1,\n",
    "                                padding = 1))\n",
    "    \n",
    "    self.block_5 = nn.ModuleList([\n",
    "        nn.Conv2d(in_channels = 512,\n",
    "                              out_channels = 512,\n",
    "                              kernel_size = (3, 3),\n",
    "                              stride = 1,\n",
    "                              padding = 1),\n",
    "        nn.Conv2d(in_channels = 512,\n",
    "                          out_channels = 512,\n",
    "                          kernel_size = (3, 3),\n",
    "                          stride = 1,\n",
    "                          padding = 1),\n",
    "        nn.MaxPool2d(kernel_size = (2, 2),\n",
    "                    stride = 2)\n",
    "    ])\n",
    "    if version > 13:\n",
    "      self.block_5.insert(2, nn.Conv2d(in_channels = 512,\n",
    "                          out_channels = 512,\n",
    "                          kernel_size = (3, 3),\n",
    "                          stride = 1,\n",
    "                          padding = 1))\n",
    "    if version > 16:\n",
    "      self.block_5.insert(2, nn.Conv2d(in_channels = 512,\n",
    "                          out_channels = 512,\n",
    "                          kernel_size = (3, 3),\n",
    "                          stride = 1,\n",
    "                          padding = 1))\n",
    "\n",
    "    self.linear_block = nn.Sequential(\n",
    "                                       nn.Flatten(),\n",
    "                                       nn.Linear(in_features = 512*H//(2**(num_pool))*W//(2**(num_pool)),\n",
    "                                                 out_features = 4096),\n",
    "                                       nn.Linear(in_features = 4096,\n",
    "                                                 out_features = 4096),\n",
    "                                       nn.Linear(in_features = 4096,\n",
    "                                                 out_features = 1000)\n",
    "    )\n",
    "            \n",
    "  def forward(self, x):\n",
    "    for module in self.block_1:\n",
    "      x = module(x)\n",
    "    print(x.size())\n",
    "\n",
    "    for module in self.block_2:\n",
    "      x = module(x)\n",
    "    print(x.size())\n",
    "\n",
    "    for module in self.block_3:\n",
    "      x = module(x)\n",
    "    print(x.size())\n",
    "\n",
    "    for module in self.block_4:\n",
    "      x = module(x)\n",
    "    print(x.size())\n",
    "\n",
    "    for module in self.block_5:\n",
    "      x = module(x)\n",
    "    print(x.size())\n",
    "\n",
    "    x = self.linear_block(x)\n",
    "    print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YbIQCSwppCeJ"
   },
   "outputs": [],
   "source": [
    "x = torch.randn(1, 3, 224, 224)\n",
    "model = VGG(19, x.size(1), x.size(2), x.size(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AZv_vDzf1pNY",
    "outputId": "08eabe59-0317-40b8-dcb3-d524ac4a0642"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (block_1): ModuleList(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (2): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (block_2): ModuleList(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (2): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (block_3): ModuleList(\n",
       "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (block_4): ModuleList(\n",
       "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (block_5): ModuleList(\n",
       "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (linear_block): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (2): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (3): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M_JsHMjfr_DL",
    "outputId": "92218e99-7d96-4ce6-e8f0-c05fb49bfa44",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 112, 112])\n",
      "torch.Size([1, 128, 56, 56])\n",
      "torch.Size([1, 256, 28, 28])\n",
      "torch.Size([1, 512, 14, 14])\n",
      "torch.Size([1, 512, 7, 7])\n",
      "torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "model(x)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "VGG.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
